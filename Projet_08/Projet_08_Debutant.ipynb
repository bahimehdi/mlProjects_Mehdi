{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16872064",
   "metadata": {},
   "source": [
    "\n",
    "# üéì Projet 8 : Analyse de Sentiment en Sant√© Mentale\n",
    "## Version D√©butant - \"Je te montre, puis tu r√©p√®tes\"\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ L'Objectif de ce Projet\n",
    "\n",
    "Les r√©seaux sociaux peuvent r√©v√©ler des signes de d√©pression ou d'anxi√©t√©. Votre mission est de **d√©tecter l'√©tat mental** √† partir de posts textuels (`Normal`, `Depressed`, `Anxious`) afin de pouvoir intervenir et proposer du soutien.\n",
    "\n",
    "**Ce que vous allez apprendre :**\n",
    "- üìù Analyser des **donn√©es textuelles** (NLP - Natural Language Processing)\n",
    "- üïê Extraire des features temporelles (heure, weekend, etc.)\n",
    "- üî§ Cr√©er des features NLP (longueur, sentiment, mots n√©gatifs)\n",
    "- ü§ñ Utiliser `RandomForestClassifier` pour la **classification multi-classe**\n",
    "- üìä Comparer les performances sur 3 cat√©gories avec **F1-Score**\n",
    "\n",
    "---\n",
    "\n",
    "> **üí° Comment utiliser ce notebook :**\n",
    "> 1. **Les cellules avec du code complet** ‚Üí Lisez et ex√©cutez-les pour voir l'exemple\n",
    "> 2. **Les cellules avec # TODO** ‚Üí C'est votre tour ! R√©p√©tez la technique\n",
    "> 3. **Les Questions ‚ùì** ‚Üí R√©fl√©chissez avant de passer √† la suite\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6450b17",
   "metadata": {},
   "source": [
    "\n",
    "# üìã SESSION 1 : From Raw Data to Clean Insights (45 min)\n",
    "\n",
    "## Part 1: The Setup (10 min)\n",
    "\n",
    "### üìò Theory: Les Biblioth√®ques\n",
    "\n",
    "Nous allons utiliser des outils sp√©cialement con√ßus pour le **texte** (NLTK, TextBlob).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d92a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# NLP Libraries (on va les installer si elles manquent)\n",
    "try:\n",
    "    from textblob import TextBlob\n",
    "    print(\"‚úÖ TextBlob disponible\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è TextBlob manquant. Installez avec: pip install textblob\")\n",
    "\n",
    "print(\"‚úÖ Biblioth√®ques import√©es !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1bf822",
   "metadata": {},
   "source": [
    "\n",
    "### üõ†Ô∏è √âtape 1.1 : Charger les Donn√©es\n",
    "Le fichier est `sante_mentale.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a7ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('sante_mentale.csv')\n",
    "\n",
    "print(\"üìä Aper√ßu des donn√©es :\")\n",
    "display(df.head())\n",
    "print(f\"\\n‚úÖ Dimensions : {df.shape[0]} posts, {df.shape[1]} colonnes\")\n",
    "print(f\"\\n‚ÑπÔ∏è Colonnes : {df.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c040d39e",
   "metadata": {},
   "source": [
    "\n",
    "> **üí° Tip:** Notez la colonne `Texte` - c'est l√† que se trouve l'information principale ! \n",
    "> `Horodatage` nous permettra d'extraire l'heure, et `Plateforme` est cat√©gorique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d31432",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: The Sanity Check (15 min)\n",
    "\n",
    "### üìò Theory: Valeurs Manquantes\n",
    "V√©rifions si nous avons des trous dans nos donn√©es.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b346032",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"üîç Valeurs manquantes :\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e069a4c5",
   "metadata": {},
   "source": [
    "\n",
    "### üõ†Ô∏è Exemple : Remplir 'Texte'\n",
    "Pour le texte manquant, nous allons utiliser un texte neutre par d√©faut.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa7b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Texte'].fillna(\"No text\", inplace=True)\n",
    "print(\"‚úÖ Texte manquant rempli avec : 'No text'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c802ef62",
   "metadata": {},
   "source": [
    "\n",
    "### üõ†Ô∏è V√©rification des Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37a1b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"üîç Duplicates trouv√©s : {df.duplicated().sum()}\")\n",
    "if df.duplicated().sum() > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(\"‚úÖ Duplicates supprim√©s !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55799af",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3: Exploratory Data Analysis (20 min)\n",
    "\n",
    "### üìä Visualisation 1 : Distribution des √âtiquettes\n",
    "Combien de posts sont `Normal`, `Depressed`, `Anxious` ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3afc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "etiquette_counts = df['Etiquette'].value_counts()\n",
    "colors = ['green', 'orange', 'red']\n",
    "etiquette_counts.plot(kind='bar', color=colors)\n",
    "plt.title('Distribution des √âtiquettes de Sant√© Mentale')\n",
    "plt.xlabel('√âtiquette')\n",
    "plt.ylabel('Nombre de Posts')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPourcentages :\")\n",
    "print(df['Etiquette'].value_counts(normalize=True) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a46f60c",
   "metadata": {},
   "source": [
    "\n",
    "> **‚ö†Ô∏è Warning:** La classe `Normal` est l√©g√®rement majoritaire (~50%), mais les classes `Depressed` et `Anxious` sont importantes √† d√©tecter !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b616a85",
   "metadata": {},
   "source": [
    "\n",
    "### üõ†Ô∏è √Ä vous de jouer !\n",
    "Visualisez la r√©partition des posts par **Plateforme** avec un graphique en barres.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c49183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Graphique en barres pour Plateforme\n",
    "\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# df['Plateforme'].value_counts().plot(kind='bar', color='skyblue')\n",
    "# plt.title('R√©partition des Posts par Plateforme')\n",
    "# plt.xlabel('Plateforme')\n",
    "# plt.ylabel('Nombre de Posts')\n",
    "# plt.xticks(rotation=0)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cca9836",
   "metadata": {},
   "source": [
    "\n",
    "### ‚ùì Question de R√©flexion\n",
    "Quelle plateforme a le plus de posts ? Cela pourrait-il influencer notre mod√®le ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e72fc",
   "metadata": {},
   "source": [
    "\n",
    "### üìä Visualisation 2 : Longueur du Texte par √âtiquette\n",
    "Les posts d√©prim√©s ou anxieux sont-ils plus courts/longs ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a091d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cr√©er une feature temporaire pour la longueur\n",
    "df['Text_Length_Temp'] = df['Texte'].str.len()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for etiquette in df['Etiquette'].unique():\n",
    "    subset = df[df['Etiquette'] == etiquette]['Text_Length_Temp']\n",
    "    plt.hist(subset, alpha=0.5, label=etiquette, bins=20)\n",
    "\n",
    "plt.title('Distribution de la Longueur du Texte par √âtiquette')\n",
    "plt.xlabel('Longueur du Texte (caract√®res)')\n",
    "plt.ylabel('Fr√©quence')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "df = df.drop(columns=['Text_Length_Temp'])  # Nettoyage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bae319",
   "metadata": {},
   "source": [
    "\n",
    "# üìã SESSION 2 : The Art of Feature Engineering (45 min)\n",
    "\n",
    "## Part 1: The Concept (10 min)\n",
    "\n",
    "### üß† Qu'est-ce que le Feature Engineering ?\n",
    "\n",
    "**Analogie :** Imaginez que vous √™tes un d√©tective psychologue. Un texte brut, c'est comme une conversation enregistr√©e. Vous devez extraire des **indices** :\n",
    "- üìè Combien de mots utilisent-ils ? (Longueur)\n",
    "- üò° Y a-t-il des mots n√©gatifs ? (Sentiment)\n",
    "- üïê √Ä quelle heure ont-ils post√© ? (Contexte temporel)\n",
    "\n",
    "C'est exactement ce que nous allons faire avec le **NLP (Natural Language Processing)**.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: The Lab - Choose Your Recipe (30 min)\n",
    "\n",
    "### üìù Recipe 3: Text & NLP (PRIMARY)\n",
    "\n",
    "#### üìò Theory: Features Textuelles de Base\n",
    "\n",
    "Commen√ßons simple :\n",
    "- **Text_Length** : Nombre de caract√®res\n",
    "- **Word_Count** : Nombre de mots\n",
    "- **Avg_Word_Length** : Longueur moyenne des mots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c956cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exemple complet pour Text_Length\n",
    "df['Text_Length'] = df['Texte'].str.len()\n",
    "print(\"‚úÖ Feature Text_Length cr√©√©e !\")\n",
    "print(df[['Texte', 'Text_Length']].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c5e368",
   "metadata": {},
   "source": [
    "\n",
    "### üõ†Ô∏è √Ä vous de jouer !\n",
    "Cr√©ez la feature **Word_Count** (nombre de mots dans `Texte`).\n",
    "\n",
    "**Astuce :** Utilisez `.str.split().str.len()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5cc6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Cr√©er Word_Count\n",
    "\n",
    "# df['Word_Count'] = df['Texte'].str.split().str.len()\n",
    "# print(\"‚úÖ Feature Word_Count cr√©√©e !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b57c9e",
   "metadata": {},
   "source": [
    "\n",
    "#### üìò Theory: Analyse de Sentiment avec TextBlob\n",
    "\n",
    "**TextBlob** peut analyser le **sentiment** (positif/n√©gatif) et la **subjectivit√©** (opinion vs fait).\n",
    "\n",
    "- **Polarity** : -1 (tr√®s n√©gatif) ‚Üí +1 (tr√®s positif)\n",
    "- **Subjectivity** : 0 (objectif) ‚Üí 1 (subjectif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b3b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Exemple pour Sentiment_Polarity\n",
    "def get_polarity(text):\n",
    "    try:\n",
    "        return TextBlob(str(text)).sentiment.polarity\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "df['Sentiment_Polarity'] = df['Texte'].apply(get_polarity)\n",
    "print(\"‚úÖ Feature Sentiment_Polarity cr√©√©e !\")\n",
    "print(df[['Texte', 'Sentiment_Polarity']].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100f47cd",
   "metadata": {},
   "source": [
    "\n",
    "### üõ†Ô∏è √Ä vous de jouer !\n",
    "Cr√©ez **Sentiment_Subjectivity** en adaptant la fonction ci-dessus.\n",
    "\n",
    "**Astuce :** Remplacez `.sentiment.polarity` par `.sentiment.subjectivity`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0958ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Cr√©er Sentiment_Subjectivity\n",
    "\n",
    "# def get_subjectivity(text):\n",
    "#     try:\n",
    "#         return TextBlob(str(text)).sentiment.subjectivity\n",
    "#     except:\n",
    "#         return 0\n",
    "\n",
    "# df['Sentiment_Subjectivity'] = df['Texte'].apply(get_subjectivity)\n",
    "# print(\"‚úÖ Feature Sentiment_Subjectivity cr√©√©e !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52cdddd",
   "metadata": {},
   "source": [
    "\n",
    "#### üéØ Recipe 6: Domain-Specific Features (Mental Health)\n",
    "\n",
    "**Contexte M√©tier :** Pour la sant√© mentale, certains mots sont des **indicateurs forts** :\n",
    "- Mots n√©gatifs : \"sad\", \"alone\", \"hopeless\", \"tired\"\n",
    "- Mots urgents : \"suicide\", \"hurt myself\", \"end it\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9be839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Liste de mots n√©gatifs\n",
    "negative_words = ['sad', 'alone', 'hopeless', 'tired', 'depressed', 'anxious', 'worry', 'fear', 'bad', 'awful']\n",
    "\n",
    "# Compter les mots n√©gatifs\n",
    "def count_negative_words(text):\n",
    "    text_lower = str(text).lower()\n",
    "    return sum([1 for word in negative_words if word in text_lower])\n",
    "\n",
    "df['Negative_Word_Count'] = df['Texte'].apply(count_negative_words)\n",
    "df['Has_Negative_Words'] = (df['Negative_Word_Count'] > 0).astype(int)\n",
    "\n",
    "print(\"‚úÖ Features Negative_Word_Count et Has_Negative_Words cr√©√©es !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e995c870",
   "metadata": {},
   "source": [
    "\n",
    "### üõ†Ô∏è √Ä vous de jouer !\n",
    "Cr√©ez une feature **Has_Urgent_Keywords** pour d√©tecter les mots li√©s aux id√©es suicidaires.\n",
    "\n",
    "**Liste sugg√©r√©e :** `['suicide', 'kill', 'die', 'death', 'end it', 'hurt myself']`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac244349",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Cr√©er Has_Urgent_Keywords\n",
    "\n",
    "# urgent_words = ['suicide', 'kill', 'die', 'death', 'end it', 'hurt myself']\n",
    "# def has_urgent(text):\n",
    "#     text_lower = str(text).lower()\n",
    "#     return int(any(word in text_lower for word in urgent_words))\n",
    "\n",
    "# df['Has_Urgent_Keywords'] = df['Texte'].apply(has_urgent)\n",
    "# print(\"‚úÖ Feature Has_Urgent_Keywords cr√©√©e !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b089bea9",
   "metadata": {},
   "source": [
    "\n",
    "### üïê Recipe 1: Dates & Time\n",
    "\n",
    "#### üìò Theory: Extraction de Features Temporelles\n",
    "\n",
    "L'heure du post peut indiquer l'√©tat mental (posts nocturnes, weekends, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f775e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convertir Horodatage en datetime\n",
    "df['Horodatage'] = pd.to_datetime(df['Horodatage'])\n",
    "\n",
    "# Extraire Hour\n",
    "df['Hour'] = df['Horodatage'].dt.hour\n",
    "print(\"‚úÖ Feature Hour cr√©√©e !\")\n",
    "print(df[['Horodatage', 'Hour']].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57e0ee6",
   "metadata": {},
   "source": [
    "\n",
    "### üõ†Ô∏è √Ä vous de jouer !\n",
    "Cr√©ez les features suivantes :\n",
    "1. **Is_Weekend** : 1 si samedi/dimanche, 0 sinon (DayOfWeek >= 5)\n",
    "2. **Is_Night** : 1 si entre 22h et 6h, 0 sinon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b4f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Cr√©er Is_Weekend et Is_Night\n",
    "\n",
    "# df['DayOfWeek'] = df['Horodatage'].dt.dayofweek\n",
    "# df['Is_Weekend'] = (df['DayOfWeek'] >= 5).astype(int)\n",
    "\n",
    "# df['Is_Night'] = ((df['Hour'] >= 22) | (df['Hour'] <= 6)).astype(int)\n",
    "# print(\"‚úÖ Features Is_Weekend et Is_Night cr√©√©es !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5e9346",
   "metadata": {},
   "source": [
    "\n",
    "### üè∑Ô∏è Recipe 2: Categories\n",
    "\n",
    "Encodons la **Plateforme** avec One-Hot Encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64888db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.get_dummies(df, columns=['Plateforme'], prefix='Platform')\n",
    "print(\"‚úÖ Encodage de Plateforme termin√© !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac75d196",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3: Final Prep (5 min)\n",
    "\n",
    "### üßπ Nettoyage Final\n",
    "Supprimons les colonnes inutiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bdb3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Supprimer les colonnes non n√©cessaires\n",
    "columns_to_drop = ['ID_Post', 'Texte', 'Horodatage']\n",
    "# Optionnel : garder DayOfWeek ou le supprimer si d√©j√† transform√©\n",
    "if 'DayOfWeek' in df.columns:\n",
    "    columns_to_drop.append('DayOfWeek')\n",
    "\n",
    "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "print(f\"‚úÖ Colonnes restantes : {df.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6c14f8",
   "metadata": {},
   "source": [
    "\n",
    "### üéØ Pr√©parer X et y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1bc589",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop(columns=['Etiquette'])\n",
    "y = df['Etiquette']\n",
    "\n",
    "print(f\"‚úÖ Pr√™t ! X shape: {X.shape}, y shape: {y.shape}\")\n",
    "print(f\"Features : {X.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5add3e9f",
   "metadata": {},
   "source": [
    "\n",
    "# üìã SESSION 3 : Building & Trusting Your Model (45 min)\n",
    "\n",
    "## Part 1: The Split (10 min)\n",
    "\n",
    "Pour la classification multi-classe, nous utilisons `stratify=y` pour garder les proportions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c507093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Split stratifi√© effectu√© !\")\n",
    "print(f\"Train : {X_train.shape}, Test : {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b974fa18",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: Training (15 min)\n",
    "\n",
    "### ü§ñ RandomForestClassifier pour Multi-Classe\n",
    "Pas besoin de modification sp√©ciale. RandomForest g√®re automatiquement les 3 classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3cfc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "print(\"üöÄ Entra√Ænement...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"‚úÖ Mod√®le entra√Æn√© !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21494cc",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3: Evaluation (20 min)\n",
    "\n",
    "### üìä M√©triques pour Multi-Classe\n",
    "\n",
    "- **Accuracy** : Pourcentage global de pr√©dictions correctes\n",
    "- **F1-Score** : Moyenne harmonique de Pr√©cision et Recall (par classe)\n",
    "- **Confusion Matrix** : Voir o√π le mod√®le se trompe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f812fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"üéØ ACCURACY : {accuracy:.2%}\")\n",
    "print(\"\\nüìä Rapport complet (par classe) :\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f9f4df",
   "metadata": {},
   "source": [
    "\n",
    "> **üí° Tip:** Regardez le **F1-Score** pour `Depressed` et `Anxious`. Ce sont les classes les plus importantes √† d√©tecter !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab0114b",
   "metadata": {},
   "source": [
    "\n",
    "### üõ†Ô∏è √Ä vous de jouer !\n",
    "Affichez la **Matrice de Confusion** pour voir les erreurs de classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f173a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Matrice de Confusion\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# cm = confusion_matrix(y_test, y_pred, labels=['Normal', 'Depressed', 'Anxious'])\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "# plt.colorbar()\n",
    "# plt.title('Matrice de Confusion')\n",
    "# plt.xlabel('Pr√©dit')\n",
    "# plt.ylabel('R√©el')\n",
    "# tick_marks = range(3)\n",
    "# plt.xticks(tick_marks, ['Normal', 'Depressed', 'Anxious'])\n",
    "# plt.yticks(tick_marks, ['Normal', 'Depressed', 'Anxious'])\n",
    "# for i in range(3):\n",
    "#     for j in range(3):\n",
    "#         plt.text(j, i, cm[i, j], ha='center', va='center', color='white' if cm[i, j] > cm.max() / 2 else 'black')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78396b72",
   "metadata": {},
   "source": [
    "\n",
    "## üéÅ Part 4: Going Further (Bonus - 15-30 mins)\n",
    "\n",
    "The main model is trained! Now let's tackle the optional challenges from the project brief.\n",
    "\n",
    "### Bonus Task 1: Analyser les Tendances d'Humeur par Moment de la Journ√©e\n",
    "\n",
    "**Goal:** Voir si certaines heures sont associ√©es √† plus de d√©pression ou d'anxi√©t√©.\n",
    "\n",
    "**Why it matters:** Cela peut aider √† d√©ployer des interventions cibl√©es (ex: chatbot de soutien actif la nuit).\n",
    "\n",
    "**Approach:**\n",
    "1. Grouper les posts par `Hour` et `Etiquette`\n",
    "2. Compter les occurrences\n",
    "3. Visualiser avec un graphique en lignes ou heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca2961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Tendances par heure\n",
    "# Recr√©ez la colonne Hour si n√©cessaire √† partir des donn√©es originales\n",
    "# ou sauvegardez-la avant Session 2\n",
    "\n",
    "# df_original = pd.read_csv('sante_mentale.csv')\n",
    "# df_original['Horodatage'] = pd.to_datetime(df_original['Horodatage'])\n",
    "# df_original['Hour'] = df_original['Horodatage'].dt.hour\n",
    "\n",
    "# mood_by_hour = df_original.groupby(['Hour', 'Etiquette']).size().unstack(fill_value=0)\n",
    "# mood_by_hour.plot(kind='line', figsize=(12, 6), marker='o')\n",
    "# plt.title('Tendances d\\'Humeur par Heure de la Journ√©e')\n",
    "# plt.xlabel('Heure')\n",
    "# plt.ylabel('Nombre de Posts')\n",
    "# plt.legend(title='√âtiquette')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab1a00f",
   "metadata": {},
   "source": [
    "\n",
    "### Bonus Task 2: Identifier les Mots D√©clencheurs pour Chaque Cat√©gorie\n",
    "\n",
    "**Goal:** Quels mots apparaissent le plus souvent dans les posts `Depressed` vs `Anxious` vs `Normal` ?\n",
    "\n",
    "**Why it matters:** Comprendre le vocabulaire associ√© √† chaque √©tat mental pour affiner les interventions.\n",
    "\n",
    "**Approach:**\n",
    "1. S√©parer le texte par √©tiquette\n",
    "2. Compter les mots les plus fr√©quents (apr√®s nettoyage : minuscules, stopwords)\n",
    "3. Afficher les Top 10 mots par cat√©gorie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42490be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Mots d√©clencheurs\n",
    "# from collections import Counter\n",
    "# import re\n",
    "\n",
    "# df_original = pd.read_csv('sante_mentale.csv')\n",
    "\n",
    "# def get_top_words(df, label, top_n=10):\n",
    "#     texts = df[df['Etiquette'] == label]['Texte'].dropna()\n",
    "#     all_words = []\n",
    "#     for text in texts:\n",
    "#         words = re.findall(r'\\b[a-z]+\\b', text.lower())  # Mots en minuscules\n",
    "#         all_words.extend(words)\n",
    "#     \n",
    "#     # Retirer les stopwords basiques (optionnel, pour simplifier)\n",
    "#     stopwords = ['i', 'and', 'the', 'a', 'to', 'is', 'it', 'of', 'in', 'for', 'on', 'with']\n",
    "#     filtered_words = [w for w in all_words if w not in stopwords and len(w) > 2]\n",
    "#     \n",
    "#     return Counter(filtered_words).most_common(top_n)\n",
    "\n",
    "# for label in ['Normal', 'Depressed', 'Anxious']:\n",
    "#     print(f\"\\nüîë Top 10 mots pour {label}:\")\n",
    "#     print(get_top_words(df_original, label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de94b6",
   "metadata": {},
   "source": [
    "\n",
    "### Bonus Task 3: Regrouper les Utilisateurs en Groupes de Soutien\n",
    "\n",
    "**Goal:** Utiliser le clustering pour identifier des profils similaires (ex: \"Anxieux nocturnes\", \"D√©prim√©s le week-end\").\n",
    "\n",
    "**Why it matters:** Cr√©er des groupes de soutien homog√®nes pour une meilleure entraide.\n",
    "\n",
    "**Approach:**\n",
    "1. Utiliser KMeans sur les features `Sentiment_Polarity`, `Negative_Word_Count`, `Is_Night`\n",
    "2. Cr√©er 3-4 clusters\n",
    "3. Analyser la composition de chaque cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df3de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Clustering KMeans\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# # Cr√©er un dataframe avec les features importantes\n",
    "# features_for_clustering = df[['Sentiment_Polarity', 'Negative_Word_Count', 'Is_Night']].copy()\n",
    "# # (Assurez-vous que ces features existent dans df)\n",
    "\n",
    "# kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "# df['Support_Group'] = kmeans.fit_predict(features_for_clustering)\n",
    "\n",
    "# print(\"‚úÖ Clustering effectu√© !\")\n",
    "# print(\"\\nDistribution des groupes :\")\n",
    "# print(df['Support_Group'].value_counts())\n",
    "\n",
    "# # Analyser chaque groupe\n",
    "# for group in range(3):\n",
    "#     print(f\"\\n--- Groupe {group} ---\")\n",
    "#     subset = df[df['Support_Group'] == group]\n",
    "#     print(f\"Taille : {len(subset)}\")\n",
    "#     print(f\"Sentiment moyen : {subset['Sentiment_Polarity'].mean():.2f}\")\n",
    "#     print(f\"% de posts nocturnes : {subset['Is_Night'].mean() * 100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0db5f92",
   "metadata": {},
   "source": [
    "\n",
    "### Bonus Task 4: D√©tecter les Cas Urgents\n",
    "\n",
    "**Goal:** Cr√©er un syst√®me d'alerte pour les posts contenant des mots-cl√©s d'id√©es suicidaires.\n",
    "\n",
    "**Why it matters:** Priorit√© absolue - sauver des vies en d√©tectant les signaux de danger imm√©diat.\n",
    "\n",
    "**Approach:**\n",
    "1. Utiliser la feature `Has_Urgent_Keywords` cr√©√©e pr√©c√©demment\n",
    "2. Filtrer les posts urgents\n",
    "3. Afficher un tableau r√©capitulatif avec recommandations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55fc501",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Syst√®me d'alerte urgente\n",
    "# df_original = pd.read_csv('sante_mentale.csv')\n",
    "\n",
    "# urgent_words = ['suicide', 'kill', 'die', 'death', 'end it', 'hurt myself']\n",
    "# def has_urgent(text):\n",
    "#     text_lower = str(text).lower()\n",
    "#     return int(any(word in text_lower for word in urgent_words))\n",
    "\n",
    "# df_original['Has_Urgent_Keywords'] = df_original['Texte'].apply(has_urgent)\n",
    "\n",
    "# urgent_cases = df_original[df_original['Has_Urgent_Keywords'] == 1]\n",
    "\n",
    "# print(f\"üö® ALERTE : {len(urgent_cases)} cas urgents d√©tect√©s !\")\n",
    "# print(\"\\nD√©tails des cas urgents :\")\n",
    "# display(urgent_cases[['ID_Post', 'Texte', 'Etiquette', 'Horodatage']])\n",
    "\n",
    "# print(\"\\nüìû RECOMMANDATION : Ces posts n√©cessitent une intervention imm√©diate (contact ligne de pr√©vention du suicide).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eea753",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üéâ F√©licitations !\n",
    "\n",
    "Vous avez termin√© le Projet 8 ! Vous savez maintenant :\n",
    "- ‚úÖ Extraire des features NLP (longueur, sentiment, mots-cl√©s)\n",
    "- ‚úÖ Analyser des donn√©es temporelles\n",
    "- ‚úÖ Classifier des √©tats mentaux avec Machine Learning\n",
    "- ‚úÖ D√©tecter des cas urgents pour sauver des vies\n",
    "\n",
    "**Next Steps :**\n",
    "- Testez d'autres mod√®les (Logistic Regression, SVM)\n",
    "- Ajoutez plus de mots-cl√©s contextuels\n",
    "- Cr√©ez une interface web pour le syst√®me d'alerte\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
