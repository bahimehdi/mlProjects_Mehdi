import nbformat as nbf
import os

def create_notebook():
    nb = nbf.v4.new_notebook()
    
    # --- CELLULES DU NOTEBOOK ---
    
    # TITRE ET INTRODUCTION
    cells = [
        nbf.v4.new_markdown_cell("""# üéì PROJET 15 : Optimiseur d'Annulation d'H√¥tel
## üèÅ Objectif du Projet
Les annulations de derni√®re minute co√ªtent cher aux h√¥tels. Votre mission est de pr√©dire si un client va annuler sa r√©servation (`Annule = 1`) ou non (`Annule = 0`).
Cela permettra √† l'h√¥tel de mieux g√©rer ses chambres et d'optimiser son taux d'occupation.

## üìÇ Les Donn√©es
Le fichier `annulation_hotel.csv` contient 800 r√©servations avec des informations comme le d√©lai de r√©servation, le prix, le segment de march√©, etc.

---
# üìã SESSION 1 : From Raw Data to Clean Insights (45 min)
"""),
        
        # PART 1: THE SETUP
        nbf.v4.new_markdown_cell("""## üõ†Ô∏è Part 1: The Setup (10 min)
Commen√ßons par importer les outils n√©cessaires et charger nos donn√©es."""),
        
        nbf.v4.new_code_cell("""import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Configuration pour un affichage plus joli
sns.set_theme(style="whitegrid")
pd.set_option('display.max_columns', None)

print("‚úÖ Biblioth√®ques import√©es avec succ√®s !")"""),
        
        nbf.v4.new_markdown_cell("""### üì• Chargement des donn√©es"""),
        
        nbf.v4.new_code_cell("""# Chargement du dataset
df = pd.read_csv('annulation_hotel.csv')

# Aper√ßu des 5 premi√®res lignes
print("Aper√ßu des donn√©es :")
display(df.head())

# Informations sur les colonnes
print("\\nInfos du dataset :")
df.info()"""),
        
        nbf.v4.new_markdown_cell("""‚ùì **Question :** Regardez la colonne `Annule`. Quelles sont les deux valeurs possibles ? Que signifient-elles ?"""),
        
        # PART 2: THE SANITY CHECK
        nbf.v4.new_markdown_cell("""## üßπ Part 2: The Sanity Check (15 min)
Avant d'analyser, il faut nettoyer ! V√©rifions les valeurs manquantes et les doublons."""),
        
        nbf.v4.new_markdown_cell("""### üîç Valeurs Manquantes"""),
        
        nbf.v4.new_code_cell("""# V√©rification des valeurs manquantes
print("Valeurs manquantes par colonne :")
print(df.isnull().sum())

# Visualisation des manquants
plt.figure(figsize=(10, 6))
sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
plt.title('Carte des valeurs manquantes')
plt.show()"""),
        
        nbf.v4.new_markdown_cell("""üìò **Theory :** Les valeurs manquantes peuvent fausser nos mod√®les.
- Pour les colonnes num√©riques (ex: `Prix_Moyen`), on remplace souvent par la **m√©diane**.
- Pour les colonnes cat√©gorielles (ex: `Segment_Marche`), on remplace par le **mode** (la valeur la plus fr√©quente) ou "Inconnu"."""),
        
        nbf.v4.new_code_cell("""# 1. Traitement de 'Prix_Moyen' (Num√©rique) -> M√©diane
mediane_prix = df['Prix_Moyen'].median()
df['Prix_Moyen'].fillna(mediane_prix, inplace=True)
print(f"‚úÖ 'Prix_Moyen' rempli avec la m√©diane : {mediane_prix}")

# TODO: R√©p√©tez pour 'Demandes_Speciales' (Num√©rique) -> M√©diane
# Votre code ici

# 2. Traitement de 'Segment_Marche' (Cat√©goriel) -> Mode
mode_segment = df['Segment_Marche'].mode()[0]
df['Segment_Marche'].fillna(mode_segment, inplace=True)
print(f"‚úÖ 'Segment_Marche' rempli avec le mode : {mode_segment}")

# V√©rification finale
assert df.isnull().sum().sum() == 0, "‚ö†Ô∏è Il reste des valeurs manquantes !"
print("‚úÖ Plus aucune valeur manquante !")"""),
        
        nbf.v4.new_markdown_cell("""### üëØ Doublons"""),
        
        nbf.v4.new_code_cell("""# V√©rification et suppression des doublons
doublons = df.duplicated().sum()
print(f"Nombre de doublons d√©tect√©s : {doublons}")

if doublons > 0:
    df.drop_duplicates(inplace=True)
    print(f"‚úÖ {doublons} doublons supprim√©s.")
else:
    print("‚úÖ Aucun doublon trouv√©.")"""),
        
        # PART 3: EDA
        nbf.v4.new_markdown_cell("""## üìä Part 3: Exploratory Data Analysis (20 min)
Explorons nos donn√©es pour comprendre ce qui influence les annulations."""),
        
        nbf.v4.new_markdown_cell("""### üéØ Analyse de la Cible (`Annule`)"""),
        
        nbf.v4.new_code_cell("""# Distribution des annulations
plt.figure(figsize=(6, 4))
sns.countplot(x='Annule', data=df, palette='pastel')
plt.title('Distribution des Annulations (0=Non, 1=Oui)')
plt.xlabel('Annul√© ?')
plt.ylabel('Nombre de r√©servations')
plt.show()

# Pourcentage
print(df['Annule'].value_counts(normalize=True) * 100)"""),
        
        nbf.v4.new_markdown_cell("""‚ùì **Question :** Y a-t-il plus d'annulations ou de s√©jours confirm√©s ? Est-ce √©quilibr√© ?"""),
        
        nbf.v4.new_markdown_cell("""### ‚è±Ô∏è D√©lai de R√©servation vs Annulation"""),
        
        nbf.v4.new_code_cell("""# Boxplot du D√©lai de R√©servation selon l'Annulation
plt.figure(figsize=(8, 5))
sns.boxplot(x='Annule', y='Delai_Reservation', data=df, palette='Set2')
plt.title('Impact du D√©lai de R√©servation sur l\'Annulation')
plt.show()"""),
        
        nbf.v4.new_markdown_cell("""‚ùì **Question :** Les gens qui r√©servent tr√®s longtemps √† l'avance annulent-ils plus souvent ?"""),
        
        nbf.v4.new_markdown_cell("""---
# üß™ SESSION 2 : The Art of Feature Engineering (45 min)
Nous allons cr√©er de nouvelles variables pour aider notre mod√®le."""),
        
        nbf.v4.new_markdown_cell("""## üç≥ Recipe 2: Categories üè∑Ô∏è
Les ordinateurs ne comprennent pas le texte comme "Online" ou "Corporate". Nous devons encoder ces cat√©gories."""),
        
        nbf.v4.new_code_cell("""# Encodage One-Hot pour 'Segment_Marche'
df = pd.get_dummies(df, columns=['Segment_Marche'], drop_first=True)

print("‚úÖ Colonnes apr√®s encodage :")
print(df.columns.tolist())
display(df.head())"""),
        
        nbf.v4.new_markdown_cell("""## üç≥ Recipe 6: Domain-Specific Features üéØ
Cr√©ons des variables sp√©cifiques √† l'h√¥tellerie."""),
        
        nbf.v4.new_markdown_cell("""### 1. Cat√©gorie de D√©lai (Lead Time Category)
Classons les r√©servations en "Derni√®re minute", "Normal", "Planifi√©"."""),
        
        nbf.v4.new_code_cell("""def categorize_lead_time(days):
    if days <= 7:
        return 'Last_Minute'
    elif days <= 30:
        return 'Normal'
    else:
        return 'Planned'

df['Lead_Time_Category'] = df['Delai_Reservation'].apply(categorize_lead_time)

# V√©rifions la relation avec l'annulation
plt.figure(figsize=(8, 5))
sns.countplot(x='Lead_Time_Category', hue='Annule', data=df, order=['Last_Minute', 'Normal', 'Planned'])
plt.title('Annulations par Cat√©gorie de D√©lai')
plt.show()

# Encodage de cette nouvelle variable
df = pd.get_dummies(df, columns=['Lead_Time_Category'], drop_first=True)
print("‚úÖ Feature 'Lead_Time_Category' cr√©√©e et encod√©e !")"""),
        
        nbf.v4.new_markdown_cell("""### 2. Demandes Sp√©ciales (Indicateur d'engagement)
Un client qui fait des demandes sp√©ciales est peut-√™tre plus engag√©."""),
        
        nbf.v4.new_code_cell("""# Cr√©ons une variable binaire : A fait une demande ou non
df['Has_Requests'] = (df['Demandes_Speciales'] > 0).astype(int)

# Visualisons
sns.barplot(x='Has_Requests', y='Annule', data=df)
plt.title('Taux d\'annulation selon s\'il y a des demandes sp√©ciales')
plt.show()"""),
        
        nbf.v4.new_markdown_cell("""---
# ü§ñ SESSION 3 : Building & Trusting Your Model (45 min)
C'est le moment de pr√©dire !"""),
        
        nbf.v4.new_markdown_cell("""## ‚úÇÔ∏è Part 1: The Split (10 min)
S√©parons les donn√©es pour l'entra√Ænement et le test."""),
        
        nbf.v4.new_code_cell("""from sklearn.model_selection import train_test_split

# D√©finition des features (X) et de la cible (y)
X = df.drop(['ID_Reservation', 'Annule'], axis=1) # On enl√®ve l'ID qui ne sert √† rien
y = df['Annule']

# Split 80% train, 20% test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"‚úÖ Donn√©es divis√©es : {X_train.shape[0]} entra√Ænement, {X_test.shape[0]} test")"""),
        
        nbf.v4.new_markdown_cell("""## üèãÔ∏è Part 2: Training & Calibration (15 min)
### üìà CAS 3 : Classification avec Calibration des Probabilit√©s
Pour g√©rer le surbooking, nous avons besoin de **probabilit√©s fiables**, pas juste d'une pr√©diction Oui/Non.
Nous allons utiliser un `RandomForestClassifier` et le calibrer."""),
        
        nbf.v4.new_code_cell("""from sklearn.ensemble import RandomForestClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import roc_auc_score, brier_score_loss

# 1. Entra√Æner le mod√®le de base
base_model = RandomForestClassifier(n_estimators=100, random_state=42)
base_model.fit(X_train, y_train)

# 2. Calibrer les probabilit√©s
# Cela ajuste le mod√®le pour que "70% de probabilit√©" signifie vraiment "70% de chance d'annuler"
calibrated_model = CalibratedClassifierCV(base_model, method='sigmoid', cv=5)
calibrated_model.fit(X_train, y_train)

print("‚úÖ Mod√®le entra√Æn√© et calibr√© !")"""),
        
        nbf.v4.new_markdown_cell("""## üéØ Part 3: Evaluation (20 min)
V√©rifions la qualit√© de nos probabilit√©s."""),
        
        nbf.v4.new_code_cell("""# Pr√©diction des probabilit√©s sur le test
y_pred_proba = calibrated_model.predict_proba(X_test)[:, 1]

# √âvaluation
auc = roc_auc_score(y_test, y_pred_proba)
brier = brier_score_loss(y_test, y_pred_proba)

print(f"üéØ ROC-AUC Score : {auc:.3f} (Plus proche de 1 est mieux)")
print(f"üéØ Brier Score : {brier:.3f} (Plus proche de 0 est mieux)")

# Visualisation de la distribution des probabilit√©s
plt.figure(figsize=(8, 5))
sns.histplot(y_pred_proba, bins=20, kde=True)
plt.title('Distribution des Probabilit√©s d\'Annulation Pr√©dites')
plt.xlabel('Probabilit√© d\'Annulation')
plt.show()"""),
        
        nbf.v4.new_markdown_cell("""## üéÅ Part 4: Going Further (Bonus)
Utilisons nos pr√©dictions pour prendre des d√©cisions business !"""),
        
        nbf.v4.new_markdown_cell("""### üè® Bonus Task 1: Calculate Optimal Overbooking Limit
**Goal:** Recommander combien de chambres suppl√©mentaires l'h√¥tel peut vendre sans risque.
**Why it matters:** Si l'h√¥tel ne surr√©serve pas, il perd de l'argent sur les annulations. S'il surr√©serve trop, il doit reloger des clients (tr√®s cher).

**Approche :**
1. Calculer le nombre attendu d'annulations (Somme des probabilit√©s).
2. Appliquer une marge de s√©curit√© (ex: 80% de confiance)."""),
        
        nbf.v4.new_code_cell("""# Imaginons que X_test repr√©sente les r√©servations du mois prochain
expected_cancellations = y_pred_proba.sum()
safe_overbooking = int(expected_cancellations * 0.8) # Marge de s√©curit√©

print(f"üìä Nombre total de r√©servations : {len(X_test)}")
print(f"üîÆ Annulations attendues (Somme des probas) : {expected_cancellations:.1f}")
print(f"‚úÖ Limite de surr√©servation recommand√©e (Safe) : {safe_overbooking} chambres")

print(f"\\nüí° Conseil : Vous pouvez vendre {safe_overbooking} chambres de plus que votre capacit√© totale !")"""),
        
        nbf.v4.new_markdown_cell("""### üë• Bonus Task 2: Segmenter les clients par Fiabilit√©
**Goal:** Identifier les clients "√Ä Risque" vs "Fiables".
**Why it matters:** On peut appeler les clients √† risque pour confirmer, et laisser tranquilles les fiables."""),
        
        nbf.v4.new_code_cell("""# Cr√©ation des segments bas√©s sur la probabilit√©
conditions = [
    (y_pred_proba < 0.3),
    (y_pred_proba >= 0.3) & (y_pred_proba < 0.7),
    (y_pred_proba >= 0.7)
]
choices = ['Fiable', 'Incertain', '√Ä Risque']

segments = np.select(conditions, choices)

# Visualisation
unique, counts = np.unique(segments, return_counts=True)
plt.figure(figsize=(6, 6))
plt.pie(counts, labels=unique, autopct='%1.1f%%', colors=['#66b3ff', '#ffcc99', '#ff9999'])
plt.title('Segmentation des R√©servations Futures')
plt.show()""")
    ]
    
    nb.cells = cells
    
    with open('notebook_debutant_projet_15.ipynb', 'w', encoding='utf-8') as f:
        nbf.write(nb, f)
    print("Notebook D√©butant g√©n√©r√© avec succ√®s !")

if __name__ == "__main__":
    create_notebook()
