import nbformat as nbf

def create_notebook():
    nb = nbf.v4.new_notebook()
    
    # --- CONFIGURATION ---
    PROJECT_NUMBER = 14
    PROJECT_TITLE = "Juste Valeur de Voiture d'Occasion"
    DATASET_NAME = "voitures_occasion.csv"
    TARGET_VARIABLE = "Price"
    
    # --- CELLULES ---
    cells = []
    
    # HEADER
    cells.append(nbf.v4.new_markdown_cell(f"""
# ğŸ“ PROJET {PROJECT_NUMBER} : {PROJECT_TITLE}

Bienvenue dans ce projet de Data Science ! Nous allons construire un modÃ¨le pour estimer le **juste prix** d'une voiture d'occasion.

**Objectifs :**
1.  Nettoyer et explorer les donnÃ©es de voitures.
2.  CrÃ©er des fonctionnalitÃ©s (features) intelligentes (Ã¢ge de la voiture, kilomÃ©trage annuel).
3.  EntraÃ®ner une Intelligence Artificielle pour prÃ©dire le prix.
4.  **BONUS :** DÃ©tecter les "Bonnes Affaires" !

---
"""))

    # --- SESSION 1 ---
    cells.append(nbf.v4.new_markdown_cell("""
# ğŸ“‹ SESSION 1 : From Raw Data to Clean Insights (45 min)

Dans cette session, nous allons prÃ©parer nos donnÃ©es pour l'analyse.
"""))

    # Part 1: Setup
    cells.append(nbf.v4.new_markdown_cell("""
## 1.1 The Setup ğŸ› ï¸
Importons les outils nÃ©cessaires et chargeons les donnÃ©es.
"""))
    
    cells.append(nbf.v4.new_code_cell(f"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Chargement des donnÃ©es
df = pd.read_csv("{DATASET_NAME}")

# Premier aperÃ§u
print("ğŸ“Š AperÃ§u des donnÃ©es :")
display(df.head())
print(f"\\nğŸ“ Dimensions : {{df.shape}}")
"""))

    # Part 2: Sanity Check
    cells.append(nbf.v4.new_markdown_cell("""
## 1.2 The Sanity Check ğŸ©º
VÃ©rifions la qualitÃ© de nos donnÃ©es. Y a-t-il des valeurs manquantes ou bizarres ?
"""))

    cells.append(nbf.v4.new_code_cell("""
# VÃ©rification des valeurs manquantes
print("ğŸ” Valeurs manquantes :")
print(df.isnull().sum())

# VÃ©rification des doublons
duplicates = df.duplicated().sum()
print(f"\\nğŸ‘¯ Doublons trouvÃ©s : {duplicates}")

# Suppression des doublons si nÃ©cessaire
if duplicates > 0:
    df = df.drop_duplicates()
    print("âœ… Doublons supprimÃ©s !")
"""))

    cells.append(nbf.v4.new_markdown_cell("""
> **ğŸ’¡ Tip:** Les doublons peuvent fausser nos statistiques. Il est toujours prudent de les retirer.
"""))

    # Part 3: EDA
    cells.append(nbf.v4.new_markdown_cell("""
## 1.3 Exploratory Data Analysis (EDA) ğŸ•µï¸â€â™€ï¸
Comprenons nos donnÃ©es avec des graphiques.

### ğŸ“Š Distribution des Prix
Quel est le prix typique d'une voiture ?
"""))

    cells.append(nbf.v4.new_code_cell(f"""
plt.figure(figsize=(10, 6))
sns.histplot(df['{TARGET_VARIABLE}'], kde=True, color='blue')
plt.title('Distribution des Prix')
plt.xlabel('Prix (â‚¬)')
plt.show()
"""))

    cells.append(nbf.v4.new_markdown_cell("""
### ğŸ› ï¸ Ã€ vous de jouer !
Analysez la relation entre le **KilomÃ©trage** et le **Prix**.
"""))

    cells.append(nbf.v4.new_code_cell(f"""
# TODO: CrÃ©ez un scatter plot (nuage de points)
# x = Kilometrage, y = {TARGET_VARIABLE}

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='Kilometrage', y='{TARGET_VARIABLE}', alpha=0.6)
plt.title('Prix vs KilomÃ©trage')
plt.show()
"""))

    cells.append(nbf.v4.new_markdown_cell("""
â“ **Question :** Que remarquez-vous ? Plus le kilomÃ©trage est Ã©levÃ©, que fait le prix ?
"""))

    # --- SESSION 2 ---
    cells.append(nbf.v4.new_markdown_cell("""
---
# ğŸ“‹ SESSION 2 : The Art of Feature Engineering (45 min)

Nous allons transformer nos donnÃ©es brutes en informations utiles pour l'IA.
"""))

    # Recipe 1: Dates (Age)
    cells.append(nbf.v4.new_markdown_cell("""
## 2.1 Recipe 1: Dates & Time ğŸ•
La colonne `Year` est utile, mais l'**Ã‚ge** de la voiture est plus parlant pour un modÃ¨le.
"""))

    cells.append(nbf.v4.new_code_cell("""
import datetime

current_year = datetime.datetime.now().year

# CrÃ©ation de la feature 'Age'
df['Age'] = current_year - df['Year']

print("âœ… Colonne 'Age' crÃ©Ã©e :")
display(df[['Year', 'Age']].head())
"""))

    # Recipe 4: Math Magic (Mileage per Year)
    cells.append(nbf.v4.new_markdown_cell("""
## 2.2 Recipe 4: Math Magic â—
Une voiture qui a beaucoup roulÃ© en peu de temps est peut-Ãªtre plus usÃ©e. CrÃ©ons `Km_par_an`.
"""))

    cells.append(nbf.v4.new_code_cell("""
# TODO: CrÃ©ez la colonne 'Km_par_an'
# Attention Ã  la division par zÃ©ro ! Si Age = 0, on peut mettre 1 ou laisser le kilomÃ©trage tel quel.

df['Km_par_an'] = df['Kilometrage'] / df['Age'].replace(0, 1)

print("âœ… Colonne 'Km_par_an' crÃ©Ã©e :")
display(df[['Kilometrage', 'Age', 'Km_par_an']].head())
"""))

    # Recipe 2: Categories (Encoding)
    cells.append(nbf.v4.new_markdown_cell("""
## 2.3 Recipe 2: Categories ğŸ·ï¸
L'ordinateur ne comprend pas "Diesel" ou "BMW". Transformons ces textes en nombres.
"""))

    cells.append(nbf.v4.new_code_cell("""
# Encodage One-Hot pour 'Fuel' et 'Brand'
df_encoded = pd.get_dummies(df, columns=['Fuel', 'Brand'], drop_first=True)

print("âœ… Encodage terminÃ© !")
display(df_encoded.head())
"""))

    cells.append(nbf.v4.new_markdown_cell("""
> **âš ï¸ Warning:** `drop_first=True` permet d'Ã©viter la redondance (ex: si ce n'est pas Diesel, c'est Essence).
"""))

    # Final Prep
    cells.append(nbf.v4.new_markdown_cell("""
## 2.4 Final Prep ğŸ
PrÃ©parons nos variables X (features) et y (target).
"""))

    cells.append(nbf.v4.new_code_cell(f"""
# Suppression des colonnes inutiles pour le modÃ¨le (ex: ID_Voiture si elle existe)
if 'ID_Voiture' in df_encoded.columns:
    df_encoded = df_encoded.drop('ID_Voiture', axis=1)

X = df_encoded.drop('{TARGET_VARIABLE}', axis=1)
y = df_encoded['{TARGET_VARIABLE}']

print("âœ… DonnÃ©es prÃªtes :")
print(f"X shape: {{X.shape}}")
print(f"y shape: {{y.shape}}")
"""))

    # --- SESSION 3 ---
    cells.append(nbf.v4.new_markdown_cell("""
---
# ğŸ“‹ SESSION 3 : Building & Trusting Your Model (45 min)

C'est le moment d'entraÃ®ner notre IA !
"""))

    # Part 1: Split
    cells.append(nbf.v4.new_markdown_cell("""
## 3.1 The Split âœ‚ï¸
SÃ©parons les donnÃ©es : 80% pour apprendre, 20% pour tester.
"""))

    cells.append(nbf.v4.new_code_cell("""
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("âœ… Split terminÃ© !")
print(f"Train set: {{X_train.shape}}")
print(f"Test set: {{X_test.shape}}")
"""))

    # Part 2: Training (Regression)
    cells.append(nbf.v4.new_markdown_cell("""
## 3.2 Training ğŸ‹ï¸â€â™‚ï¸
Nous allons utiliser un **Random Forest Regressor**. C'est un modÃ¨le puissant composÃ© de plusieurs arbres de dÃ©cision.
"""))

    cells.append(nbf.v4.new_code_cell("""
from sklearn.ensemble import RandomForestRegressor

# Initialisation du modÃ¨le
model = RandomForestRegressor(n_estimators=100, random_state=42)

# EntraÃ®nement
print("â³ EntraÃ®nement en cours...")
model.fit(X_train, y_train)
print("âœ… ModÃ¨le entraÃ®nÃ© !")
"""))

    # Part 3: Evaluation
    cells.append(nbf.v4.new_markdown_cell("""
## 3.3 Evaluation ğŸ“
Est-ce que notre modÃ¨le prÃ©dit bien les prix ?
"""))

    cells.append(nbf.v4.new_code_cell("""
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# PrÃ©dictions
y_pred = model.predict(X_test)

# MÃ©triques
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"ğŸ“‰ MAE (Erreur Moyenne) : {mae:.2f} â‚¬")
print(f"ğŸ“‰ RMSE : {rmse:.2f} â‚¬")
print(f"ğŸ“ˆ RÂ² Score (PrÃ©cision) : {r2:.2%}")
"""))

    cells.append(nbf.v4.new_markdown_cell("""
> **ğŸ’¡ Tip:** Le **RÂ²** indique Ã  quel point notre modÃ¨le explique les variations de prix. Plus il est proche de 100%, mieux c'est !
"""))

    cells.append(nbf.v4.new_code_cell("""
# Visualisation : RÃ©alitÃ© vs PrÃ©diction
plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_test, y=y_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2) # Ligne parfaite
plt.xlabel('Prix RÃ©el')
plt.ylabel('Prix PrÃ©dit')
plt.title('RÃ©alitÃ© vs PrÃ©diction')
plt.show()
"""))

    # --- PART 4: BONUS ---
    cells.append(nbf.v4.new_markdown_cell("""
---
# ğŸ Part 4: Going Further (Bonus)

Notre modÃ¨le fonctionne ! Utilisons-le pour des tÃ¢ches business concrÃ¨tes.

### Bonus Task 1: DÃ©tecteur de "Bonnes Affaires" ğŸ’

**Objectif :** Identifier les voitures vendues en dessous de leur valeur estimÃ©e.
**Pourquoi :** Pour acheter malin !

**Approche :**
1. Si `Prix_RÃ©el < Prix_PrÃ©dit * 0.9` (10% moins cher), c'est une bonne affaire.
2. Si `Prix_RÃ©el > Prix_PrÃ©dit * 1.1` (10% plus cher), c'est trop cher.
"""))

    cells.append(nbf.v4.new_code_cell("""
# CrÃ©ation d'un DataFrame de rÃ©sultats
results = pd.DataFrame({'Reel': y_test, 'Predit': y_pred})
results['Difference_Pct'] = (results['Reel'] - results['Predit']) / results['Predit']

# DÃ©finition des labels
def label_deal(row):
    if row['Difference_Pct'] < -0.10:
        return 'ğŸ’ Bonne Affaire'
    elif row['Difference_Pct'] > 0.10:
        return 'ğŸ’¸ Trop Cher'
    else:
        return 'âš–ï¸ Juste Prix'

results['Verdict'] = results.apply(label_deal, axis=1)

print("ğŸ” Exemples de verdicts :")
display(results.sample(10))

print("\\nğŸ“Š RÃ©partition des affaires :")
print(results['Verdict'].value_counts())
"""))

    cells.append(nbf.v4.new_markdown_cell("""
### Bonus Task 2: Valeur de Revente Future ğŸ“‰

**Objectif :** Estimer le prix de ces voitures dans 5 ans.
**Approche :**
1. On prend nos donnÃ©es de test.
2. On ajoute 5 ans Ã  l'Ã¢ge (`Age + 5`).
3. On demande au modÃ¨le de prÃ©dire le nouveau prix.
"""))

    cells.append(nbf.v4.new_code_cell("""
# Simulation dans 5 ans
X_future = X_test.copy()
X_future['Age'] = X_future['Age'] + 5
# On pourrait aussi augmenter le kilomÃ©trage (ex: +15000km/an * 5)
X_future['Kilometrage'] = X_future['Kilometrage'] + (15000 * 5)
# Recalcul de Km_par_an
X_future['Km_par_an'] = X_future['Kilometrage'] / X_future['Age']

# PrÃ©diction
future_price = model.predict(X_future)

# Comparaison
comparison = pd.DataFrame({
    'Prix_Actuel': y_pred,
    'Prix_Dans_5_Ans': future_price
})
comparison['Perte_Valeur'] = comparison['Prix_Actuel'] - comparison['Prix_Dans_5_Ans']

print("ğŸ“‰ Estimation de la perte de valeur sur 5 ans :")
display(comparison.head())
print(f"Perte moyenne : {comparison['Perte_Valeur'].mean():.2f} â‚¬")
"""))

    # SAVE
    with open('notebook_debutant_projet_14.ipynb', 'w', encoding='utf-8') as f:
        nbf.write(nb, f)
    print("âœ… Notebook DÃ©butant gÃ©nÃ©rÃ© avec succÃ¨s !")

if __name__ == "__main__":
    create_notebook()
