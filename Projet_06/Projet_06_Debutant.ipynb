{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118c20c8",
   "metadata": {},
   "source": [
    "\n",
    "# üì∞ Projet 6 : Classificateur de Fake News\n",
    "## Version D√©butant - \"Je te montre, puis tu r√©p√®tes\"\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ L'Objectif de ce Projet\n",
    "\n",
    "La d√©sinformation se propage plus vite que la v√©rit√© sur les r√©seaux sociaux. Votre mission est de **d√©tecter les fausses nouvelles** en analysant le titre, le contenu et les patterns de partage des articles.\n",
    "\n",
    "**Ce que vous allez apprendre :**\n",
    "- üìù Analyser des donn√©es textuelles (NLP - Natural Language Processing)\n",
    "- üîç Cr√©er des features √† partir de texte (longueur, mots-cl√©s, sentiment)\n",
    "- ü§ñ Entra√Æner un mod√®le de **Classification Binaire** (Real vs Fake)\n",
    "- üìä √âvaluer avec F1-Score et Confusion Matrix\n",
    "\n",
    "---\n",
    "\n",
    "> **üí° Comment utiliser ce notebook :**\n",
    "> 1. **Les cellules avec du code complet** ‚Üí Lisez et ex√©cutez-les pour voir l'exemple\n",
    "> 2. **Les cellules avec # TODO** ‚Üí C'est votre tour ! R√©p√©tez la technique\n",
    "> 3. **Les Questions ‚ùì** ‚Üí R√©fl√©chissez avant de passer √† la suite\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3246af39",
   "metadata": {},
   "source": [
    "\n",
    "# üìã SESSION 1 : From Raw Data to Clean Insights (45 min)\n",
    "\n",
    "## Part 1: The Setup (10 min)\n",
    "\n",
    "### üìò Theory: Les Biblioth√®ques\n",
    "Nous allons utiliser :\n",
    "- **pandas** : Pour manipuler le tableau de donn√©es\n",
    "- **numpy** : Pour les calculs math√©matiques\n",
    "- **matplotlib/seaborn** : Pour les graphiques\n",
    "- **re** : Pour analyser les patterns dans le texte (expressions r√©guli√®res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe02688",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# Configuration pour de beaux graphiques\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"‚úÖ Biblioth√®ques import√©es !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be56cb",
   "metadata": {},
   "source": [
    "\n",
    "### üõ†Ô∏è √âtape 1.1 : Charger les Donn√©es\n",
    "Le fichier s'appelle `fake_news.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62830708",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Charger le dataset\n",
    "df = pd.read_csv('fake_news.csv')\n",
    "\n",
    "# Afficher les premi√®res lignes\n",
    "print(\"üìä Aper√ßu des donn√©es :\")\n",
    "display(df.head())\n",
    "\n",
    "print(f\"\\n‚úÖ Dimensions : {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
    "print(f\"\\nüìã Colonnes : {list(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce50b1d",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: The Sanity Check (15 min)\n",
    "\n",
    "### üìò Theory: Distribution de la Cible\n",
    "Avant de construire un mod√®le, il faut v√©rifier si les classes sont √©quilibr√©es.\n",
    "Si on a 99% de vraies news et 1% de fake, le mod√®le apprendra mal !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc55415",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# V√©rifier la distribution de la cible\n",
    "print(\"üéØ Distribution des articles :\")\n",
    "print(df['Etiquette'].value_counts())\n",
    "\n",
    "# Visualiser\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(data=df, x='Etiquette', palette='Set2')\n",
    "plt.title('üìä R√©partition Real vs Fake')\n",
    "plt.ylabel('Nombre d\\'articles')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899ff3b0",
   "metadata": {},
   "source": [
    "\n",
    "### üìò Theory: Valeurs Manquantes\n",
    "Les donn√©es textuelles peuvent avoir des champs vides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537745d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# V√©rifier les valeurs manquantes\n",
    "print(\"üîç Valeurs manquantes par colonne :\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Supprimer les lignes avec texte manquant (si n√©cessaire)\n",
    "df = df.dropna(subset=['Title', 'Corps_Texte'])\n",
    "print(f\"\\n‚úÖ Dataset nettoy√© : {df.shape[0]} lignes restantes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0470df3",
   "metadata": {},
   "source": [
    "\n",
    "### üìò Theory: Duplicatas\n",
    "Parfois, le m√™me article est pr√©sent plusieurs fois.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdcd0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# V√©rifier les duplicatas\n",
    "print(f\"üîç Nombre de duplicatas : {df.duplicated().sum()}\")\n",
    "\n",
    "# Supprimer les duplicatas\n",
    "df = df.drop_duplicates()\n",
    "print(f\"‚úÖ Dataset final : {df.shape[0]} lignes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5d5287",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3: Exploratory Data Analysis (20 min)\n",
    "\n",
    "### üìä Visualisation 1 : Distribution des Partages\n",
    "Les fake news ont-elles plus de partages ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eeb14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(data=df, x='Etiquette', y='Nb_Partages', palette='coolwarm')\n",
    "plt.title('üìà Distribution des Partages : Real vs Fake')\n",
    "plt.ylabel('Nombre de Partages')\n",
    "plt.yscale('log')  # √âchelle log pour mieux voir (partages vont de 0 √† 1M)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d728f9d",
   "metadata": {},
   "source": [
    "\n",
    "### ‚ùì Question\n",
    "Les fake news ont-elles tendance √† avoir plus ou moins de partages ? Que remarquez-vous ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca14767",
   "metadata": {},
   "source": [
    "\n",
    "### üìä Visualisation 2 : Longueur du Titre\n",
    "Les fake news utilisent-elles des titres plus longs ou plus courts ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8993eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cr√©er une feature temporaire pour l'analyse\n",
    "df['Title_Length'] = df['Title'].apply(len)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(data=df, x='Title_Length', hue='Etiquette', bins=30, kde=True)\n",
    "plt.title('üìè Distribution de la Longueur des Titres')\n",
    "plt.xlabel('Nombre de caract√®res')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d64ca7",
   "metadata": {},
   "source": [
    "\n",
    "### üõ†Ô∏è √Ä vous de jouer !\n",
    "Cr√©ez une visualisation pour comparer la **longueur du texte** (`Corps_Texte`) entre Real et Fake.\n",
    "Utilisez un histogramme ou un boxplot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0240587",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Cr√©er une feature 'Body_Length' (longueur de Corps_Texte)\n",
    "\n",
    "# df['Body_Length'] = df['Corps_Texte'].apply(len)\n",
    "\n",
    "# TODO: Cr√©er un histogramme ou boxplot\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# sns.boxplot(data=df, x='Etiquette', y='Body_Length', palette='pastel')\n",
    "# plt.title('üìè Longueur du Corps de Texte : Real vs Fake')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f86b55",
   "metadata": {},
   "source": [
    "\n",
    "# üìã SESSION 2 : The Art of Feature Engineering (45 min)\n",
    "\n",
    "## Part 1: The Concept (10 min)\n",
    "\n",
    "Les mod√®les de Machine Learning ne \"lisent\" pas le texte comme nous.\n",
    "Ils ont besoin de **nombres** !\n",
    "\n",
    "Nous allons transformer le texte en features num√©riques :\n",
    "- **Statistiques** : Longueur, nombre de mots\n",
    "- **Patterns** : Pr√©sence de !!!, MAJUSCULES, chiffres\n",
    "- **Sentiment** : Ton positif/n√©gatif (avanc√©)\n",
    "\n",
    "## Part 2: The Lab - Choose Your Recipe (30 min)\n",
    "\n",
    "### üìù Recipe 3: Text & NLP\n",
    "\n",
    "#### üìò Theory: Features Statistiques\n",
    "Pour chaque texte, nous pouvons calculer :\n",
    "- Nombre de mots\n",
    "- Nombre de caract√®res\n",
    "- Longueur moyenne des mots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e396a9d",
   "metadata": {},
   "source": [
    "\n",
    "### üõ†Ô∏è Exemple : Analyser le Titre\n",
    "Cr√©ons des features pour la colonne `Title`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a535181",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature 1: Nombre de mots dans le titre\n",
    "df['Title_Word_Count'] = df['Title'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Feature 2: Nombre de caract√®res\n",
    "df['Title_Char_Count'] = df['Title'].apply(len)\n",
    "\n",
    "# Feature 3: Longueur moyenne des mots\n",
    "df['Title_Avg_Word_Length'] = df['Title'].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()) if len(x.split()) > 0 else 0)\n",
    "\n",
    "print(\"‚úÖ Features statistiques du titre cr√©√©es !\")\n",
    "display(df[['Title', 'Title_Word_Count', 'Title_Char_Count', 'Title_Avg_Word_Length']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34ea64d",
   "metadata": {},
   "source": [
    "\n",
    "### üõ†Ô∏è √Ä vous de jouer !\n",
    "R√©p√©tez la m√™me chose pour la colonne `Corps_Texte` (le corps de l'article).\n",
    "Cr√©ez 3 features : `Body_Word_Count`, `Body_Char_Count`, `Body_Avg_Word_Length`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a792d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Cr√©er des features pour Corps_Texte\n",
    "\n",
    "# df['Body_Word_Count'] = df['Corps_Texte'].apply(lambda x: len(x.split()))\n",
    "# df['Body_Char_Count'] = df['Corps_Texte'].apply(len)\n",
    "# df['Body_Avg_Word_Length'] = df['Corps_Texte'].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()) if len(x.split()) > 0 else 0)\n",
    "\n",
    "# print(\"‚úÖ Features du corps cr√©√©es !\")\n",
    "# display(df[['Corps_Texte', 'Body_Word_Count', 'Body_Char_Count']].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c3e1fe",
   "metadata": {},
   "source": [
    "\n",
    "### üìù Recipe 6: Domain-Specific Features (D√©tection de Clickbait)\n",
    "\n",
    "#### üìò Theory: Clickbait\n",
    "Les fake news utilisent souvent des titres sensationnalistes :\n",
    "- \"SHOCKING!!!\" (points d'exclamation)\n",
    "- \"YOU WON'T BELIEVE\" (tout en majuscules)\n",
    "- Chiffres exag√©r√©s\n",
    "\n",
    "Nous allons cr√©er des features pour d√©tecter ces patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6501f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature 1: Nombre de points d'exclamation\n",
    "df['Title_Exclamation_Count'] = df['Title'].apply(lambda x: x.count('!'))\n",
    "\n",
    "# Feature 2: Pourcentage de majuscules\n",
    "df['Title_Upper_Ratio'] = df['Title'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0)\n",
    "\n",
    "# Feature 3: Pr√©sence de chiffres\n",
    "df['Title_Has_Numbers'] = df['Title'].apply(lambda x: 1 if bool(re.search(r'\\d', x)) else 0)\n",
    "\n",
    "print(\"‚úÖ Features clickbait cr√©√©es !\")\n",
    "display(df[['Title', 'Title_Exclamation_Count', 'Title_Upper_Ratio', 'Title_Has_Numbers']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d4f465",
   "metadata": {},
   "source": [
    "\n",
    "### üõ†Ô∏è √Ä vous de jouer !\n",
    "Cr√©ez une feature binaire `Title_Is_Clickbait` :\n",
    "- 1 si le titre contient **3 ou plus** points d'exclamation OU **plus de 50%** de majuscules\n",
    "- 0 sinon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cca426",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Cr√©er la feature Title_Is_Clickbait\n",
    "\n",
    "# df['Title_Is_Clickbait'] = df.apply(\n",
    "#     lambda row: 1 if (row['Title_Exclamation_Count'] >= 3 or row['Title_Upper_Ratio'] > 0.5) else 0,\n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "# print(\"‚úÖ Feature Clickbait cr√©√©e !\")\n",
    "# print(df['Title_Is_Clickbait'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942316fe",
   "metadata": {},
   "source": [
    "\n",
    "### ‚ûó Recipe 4: Math Magic (Transformation des Partages)\n",
    "\n",
    "#### üìò Theory: Log Transformation\n",
    "Les `Nb_Partages` vont de 0 √† 1 million. Cette grande √©chelle peut perturber le mod√®le.\n",
    "Une **transformation logarithmique** r√©duit l'√©cart et donne plus de poids aux diff√©rences entre petits nombres.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f7283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Appliquer log(x + 1) pour √©viter log(0)\n",
    "df['Nb_Partages_Log'] = df['Nb_Partages'].apply(lambda x: np.log1p(x))\n",
    "\n",
    "print(\"‚úÖ Transformation log des partages cr√©√©e !\")\n",
    "print(f\"Avant : min={df['Nb_Partages'].min()}, max={df['Nb_Partages'].max()}\")\n",
    "print(f\"Apr√®s : min={df['Nb_Partages_Log'].min():.2f}, max={df['Nb_Partages_Log'].max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f04cd10",
   "metadata": {},
   "source": [
    "\n",
    "### üõ†Ô∏è √Ä vous de jouer !\n",
    "Cr√©ez une feature `Share_Bucket` qui cat√©gorise les articles :\n",
    "- 'Viral' si `Nb_Partages` > 10000\n",
    "- 'Popular' si entre 1000 et 10000\n",
    "- 'Low' si < 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2552c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Cr√©er la feature Share_Bucket\n",
    "\n",
    "# def categorize_shares(shares):\n",
    "#     if shares > 10000:\n",
    "#         return 'Viral'\n",
    "#     elif shares >= 1000:\n",
    "#         return 'Popular'\n",
    "#     else:\n",
    "#         return 'Low'\n",
    "\n",
    "# df['Share_Bucket'] = df['Nb_Partages'].apply(categorize_shares)\n",
    "# print(\"‚úÖ Share_Bucket cr√©√©e !\")\n",
    "# print(df['Share_Bucket'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf6cba",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3: Final Prep (5 min)\n",
    "\n",
    "Avant d'entra√Æner le mod√®le, nous devons :\n",
    "1. Encoder la cible (`Etiquette`) en 0/1\n",
    "2. S√©lectionner les features num√©riques\n",
    "3. Supprimer les colonnes textuelles originales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2e1c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encoder la cible : Real=0, Fake=1\n",
    "df['Etiquette_Encoded'] = df['Etiquette'].apply(lambda x: 1 if x == 'Fake' else 0)\n",
    "\n",
    "print(\"‚úÖ Cible encod√©e !\")\n",
    "print(df['Etiquette_Encoded'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7e8b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# S√©lectionner les features num√©riques pour le mod√®le\n",
    "feature_columns = [\n",
    "    'Title_Word_Count', 'Title_Char_Count', 'Title_Avg_Word_Length',\n",
    "    'Title_Exclamation_Count', 'Title_Upper_Ratio', 'Title_Has_Numbers',\n",
    "    'Nb_Partages_Log'\n",
    "]\n",
    "\n",
    "# V√©rifier que les colonnes existent\n",
    "available_features = [col for col in feature_columns if col in df.columns]\n",
    "print(f\"‚úÖ Features disponibles : {available_features}\")\n",
    "\n",
    "# D√©finir X (features) et y (cible)\n",
    "X = df[available_features]\n",
    "y = df['Etiquette_Encoded']\n",
    "\n",
    "print(f\"\\n‚úÖ Pr√™t pour le mod√®le ! X shape: {X.shape}, y shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d78bfe9",
   "metadata": {},
   "source": [
    "\n",
    "# üìã SESSION 3 : Building & Trusting Your Model (45 min)\n",
    "\n",
    "## Part 1: The Split (10 min)\n",
    "\n",
    "Nous divisons nos donn√©es en deux :\n",
    "- **Train (80%)** : Pour que le mod√®le apprenne\n",
    "- **Test (20%)** : Pour v√©rifier s'il a bien appris (examen final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4a1978",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"‚úÖ Train set : {X_train.shape[0]} lignes\")\n",
    "print(f\"‚úÖ Test set  : {X_test.shape[0]} lignes\")\n",
    "print(f\"\\nüìä Distribution dans Train :\")\n",
    "print(y_train.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee22bd",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: Training (15 min)\n",
    "\n",
    "Nous allons utiliser un **RandomForestClassifier**.\n",
    "C'est un mod√®le puissant qui combine plusieurs arbres de d√©cision pour voter sur la classe finale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9d836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Cr√©er le mod√®le\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Entra√Æner le mod√®le\n",
    "print(\"üöÄ Entra√Ænement en cours...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"‚úÖ Mod√®le entra√Æn√© !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699bb7b1",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3: Evaluation (20 min)\n",
    "\n",
    "### üìò Theory: M√©triques de Classification\n",
    "Pour une **Classification Binaire**, nous utilisons :\n",
    "- **Accuracy** : Pourcentage de pr√©dictions correctes\n",
    "- **F1-Score** : √âquilibre entre Pr√©cision et Rappel (id√©al pour classes d√©s√©quilibr√©es)\n",
    "- **Confusion Matrix** : Tableau montrant Vrai Positifs, Faux Positifs, etc.\n",
    "\n",
    "> **üí° Tip:** Pour la fake news, on pr√©f√®re le **F1-Score** car manquer une fake news (Faux N√©gatif) est aussi grave que bloquer une vraie news (Faux Positif).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Faire des pr√©dictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculer les m√©triques\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"üéØ Accuracy : {accuracy:.2%}\")\n",
    "print(f\"üéØ F1-Score : {f1:.3f}\")\n",
    "\n",
    "print(\"\\nüìä Rapport de Classification :\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Real', 'Fake']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653febce",
   "metadata": {},
   "source": [
    "\n",
    "### üìä Visualisation : Matrice de Confusion\n",
    "La matrice montre :\n",
    "- **Top-Left (Vrai N√©gatif)** : Real pr√©dit comme Real ‚úÖ\n",
    "- **Top-Right (Faux Positif)** : Real pr√©dit comme Fake ‚ùå\n",
    "- **Bottom-Left (Faux N√©gatif)** : Fake pr√©dit comme Real ‚ùå\n",
    "- **Bottom-Right (Vrai Positif)** : Fake pr√©dit comme Fake ‚úÖ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96145886",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cr√©er la matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
    "plt.title('üîç Matrice de Confusion')\n",
    "plt.ylabel('Vraie √âtiquette')\n",
    "plt.xlabel('Pr√©diction')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Interpr√©tation :\")\n",
    "print(f\"- Vrai N√©gatif (Real ‚Üí Real) : {cm[0, 0]}\")\n",
    "print(f\"- Faux Positif (Real ‚Üí Fake) : {cm[0, 1]} ‚ö†Ô∏è\")\n",
    "print(f\"- Faux N√©gatif (Fake ‚Üí Real) : {cm[1, 0]} ‚ö†Ô∏è\")\n",
    "print(f\"- Vrai Positif (Fake ‚Üí Fake) : {cm[1, 1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1605ed3f",
   "metadata": {},
   "source": [
    "\n",
    "### üìä Features les Plus Importantes\n",
    "Quelles features aident le plus √† d√©tecter les fake news ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd1da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extraire l'importance des features\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': available_features,\n",
    "    'Importance': model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance, x='Importance', y='Feature', palette='viridis')\n",
    "plt.title('üîë Features les Plus Importantes pour D√©tecter les Fake News')\n",
    "plt.xlabel('Importance')\n",
    "plt.show()\n",
    "\n",
    "print(feature_importance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c627250e",
   "metadata": {},
   "source": [
    "\n",
    "## üéÅ Part 4: Going Further (Bonus - 15-30 mins)\n",
    "\n",
    "Le mod√®le principal est entra√Æn√© ! Maintenant, explorons les t√¢ches secondaires du projet.\n",
    "\n",
    "### Bonus Task 1: Extraire les Mots-Cl√©s des Fake News\n",
    "\n",
    "**Goal:** Trouver les mots qui apparaissent le plus souvent dans les titres de fake news.\n",
    "\n",
    "**Why it matters:** Identifier les patterns linguistiques permet de cr√©er des filtres automatiques.\n",
    "\n",
    "**Approach:**\n",
    "1. Filtrer les articles `Etiquette == 'Fake'`\n",
    "2. Concat√©ner tous les titres en un seul texte\n",
    "3. Compter les mots les plus fr√©quents (apr√®s avoir retir√© les stop words comme \"the\", \"a\")\n",
    "\n",
    "**Deliverable:** Top 10 des mots-cl√©s dans les fake news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1901f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Extraire les mots-cl√©s des fake news\n",
    "\n",
    "# from collections import Counter\n",
    "\n",
    "# # Filtrer les fake news\n",
    "# fake_titles = df[df['Etiquette'] == 'Fake']['Title']\n",
    "\n",
    "# # Concat√©ner et splitter tous les titres\n",
    "# all_words = ' '.join(fake_titles).lower().split()\n",
    "\n",
    "# # Compter les mots (optionnel: retirer stop words)\n",
    "# word_counts = Counter(all_words)\n",
    "\n",
    "# # Afficher le top 10\n",
    "# print(\"üîë Top 10 mots dans les Fake News :\")\n",
    "# for word, count in word_counts.most_common(10):\n",
    "#     print(f\"  {word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2167994d",
   "metadata": {},
   "source": [
    "\n",
    "### Bonus Task 2: D√©tecter les Patterns \"Bot-like\"\n",
    "\n",
    "**Goal:** Identifier les articles avec un ratio partages/longueur anormalement √©lev√©.\n",
    "\n",
    "**Why it matters:** Les bots partagent massivement sans lire. Un article tr√®s court avec √©norm√©ment de partages est suspect.\n",
    "\n",
    "**Approach:**\n",
    "1. Cr√©er une feature `Share_Per_Char = Nb_Partages / Body_Char_Count`\n",
    "2. Trouver le seuil du 95e percentile\n",
    "3. Marquer les articles au-dessus comme \"Bot-like\"\n",
    "\n",
    "**Deliverable:** Liste des articles suspects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b550c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: D√©tecter les patterns bot-like\n",
    "\n",
    "# # Cr√©er le ratio partages/longueur\n",
    "# df['Share_Per_Char'] = df['Nb_Partages'] / (df['Body_Char_Count'] + 1)  # +1 pour √©viter division par 0\n",
    "\n",
    "# # Trouver le seuil (95e percentile)\n",
    "# threshold = df['Share_Per_Char'].quantile(0.95)\n",
    "\n",
    "# # Marquer les suspects\n",
    "# df['Is_Bot_Like'] = df['Share_Per_Char'] > threshold\n",
    "\n",
    "# print(f\"ü§ñ Seuil Bot-like : {threshold:.2f}\")\n",
    "# print(f\"Nombre d'articles suspects : {df['Is_Bot_Like'].sum()}\")\n",
    "\n",
    "# # Afficher quelques exemples\n",
    "# print(\"\\nExemples d'articles suspects :\")\n",
    "# display(df[df['Is_Bot_Like']][['Title', 'Nb_Partages', 'Body_Char_Count', 'Share_Per_Char']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c8c5e2",
   "metadata": {},
   "source": [
    "\n",
    "### Bonus Task 3: Pr√©dire la Viralit√© (R√©gression)\n",
    "\n",
    "**Goal:** Au lieu de classer Real/Fake, pr√©dire le **nombre de partages** qu'un article aura.\n",
    "\n",
    "**Why it matters:** Comprendre ce qui rend un contenu viral aide les cr√©ateurs de contenu l√©gitime.\n",
    "\n",
    "**Approach:**\n",
    "1. Utiliser les m√™mes features textuelles (longueur, majuscules, etc.)\n",
    "2. Changer la cible vers `Nb_Partages_Log` (pour stabiliser)\n",
    "3. Entra√Æner un **RandomForestRegressor**\n",
    "4. √âvaluer avec MAE et R¬≤\n",
    "\n",
    "**Deliverable:** Mod√®le de pr√©diction de viralit√© avec score R¬≤\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f58a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Entra√Æner un mod√®le de pr√©diction de viralit√©\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# # Pr√©parer les donn√©es (sans Nb_Partages_Log dans les features cette fois)\n",
    "# X_viral = df[['Title_Word_Count', 'Title_Char_Count', 'Title_Exclamation_Count', 'Title_Upper_Ratio']]\n",
    "# y_viral = df['Nb_Partages_Log']\n",
    "\n",
    "# # Split\n",
    "# X_train_v, X_test_v, y_train_v, y_test_v = train_test_split(X_viral, y_viral, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Entra√Æner\n",
    "# model_viral = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# model_viral.fit(X_train_v, y_train_v)\n",
    "\n",
    "# # Pr√©dire\n",
    "# y_pred_v = model_viral.predict(X_test_v)\n",
    "\n",
    "# # √âvaluer\n",
    "# mae = mean_absolute_error(y_test_v, y_pred_v)\n",
    "# r2 = r2_score(y_test_v, y_pred_v)\n",
    "\n",
    "# print(f\"üìä MAE (log scale) : {mae:.2f}\")\n",
    "# print(f\"üìä R¬≤ Score : {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4da11b",
   "metadata": {},
   "source": [
    "\n",
    "### Bonus Task 4: Topic Clustering (Regrouper par Sujet)\n",
    "\n",
    "**Goal:** Grouper les articles en cat√©gories automatiques (Politique, Sant√©, C√©l√©brit√©s).\n",
    "\n",
    "**Why it matters:** Les fake news se concentrent souvent sur certains sujets sensibles.\n",
    "\n",
    "**Approach (Avanc√©):**\n",
    "1. Utiliser TF-IDF pour vectoriser les titres\n",
    "2. Appliquer KMeans clustering (k=3 ou 5)\n",
    "3. Analyser les mots-cl√©s de chaque cluster\n",
    "\n",
    "**Deliverable:** Distribution des articles par cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be977e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Clustering par sujet (Avanc√©)\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# # Vectoriser les titres\n",
    "# vectorizer = TfidfVectorizer(max_features=50, stop_words='english')\n",
    "# X_tfidf = vectorizer.fit_transform(df['Title'])\n",
    "\n",
    "# # Clustering\n",
    "# kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "# df['Topic_Cluster'] = kmeans.fit_predict(X_tfidf)\n",
    "\n",
    "# print(\"üìö Distribution par cluster :\")\n",
    "# print(df['Topic_Cluster'].value_counts().sort_index())\n",
    "\n",
    "# # Afficher quelques exemples par cluster\n",
    "# for cluster_id in range(3):\n",
    "#     print(f\"\\n--- Cluster {cluster_id} ---\")\n",
    "#     examples = df[df['Topic_Cluster'] == cluster_id]['Title'].head(3).tolist()\n",
    "#     for ex in examples:\n",
    "#         print(f\"  - {ex}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
